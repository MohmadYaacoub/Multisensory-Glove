{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1eNeyR5PwiDAB8ma30z3SaaRXWlEFQjhc",
      "authorship_tag": "ABX9TyPy+UXlBkK9fnSbzX1lCPZ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohmadYaacoub/Data/blob/main/Data_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import** **Libraries** ##"
      ],
      "metadata": {
        "id": "giB5IkIQXAeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pickle as pkl\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "KrW8Uf_pXABB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data preparation**"
      ],
      "metadata": {
        "id": "4gPjDsMbX_5v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3S9ovvwoP6O4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45aacca4-94d0-4e48-95de-8e49fac7b9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Umbrella: 50 files\n",
            "Class Spoon: 50 files\n",
            "Class Spray: 50 files\n",
            "Class Phone Comb: 50 files\n",
            "Class Screwdriver: 50 files\n",
            "Class Plate: 50 files\n",
            "Class Remote: 50 files\n",
            "Class Scissors Comb: 50 files\n",
            "Class Shampo Comb: 50 files\n",
            "Class Scotch Comb: 50 files\n",
            "Class Coin: 50 files\n",
            "Class Pen: 50 files\n",
            "Class Clamp: 50 files\n",
            "Class Forke: 50 files\n",
            "Class Cup: 50 files\n",
            "Class Mouse: 50 files\n",
            "Class keyboard: 50 files\n",
            "Class Pepsi: 50 files\n",
            "Class Key: 50 files\n",
            "Class Orange: 50 files\n",
            "Class Calculator: 50 files\n",
            "Class Battery: 50 files\n",
            "Class Apple: 50 files\n",
            "Class Bottle of water: 50 files\n",
            "Class Agrapheuse Comb: 50 files\n",
            "Class Car remote: 50 files\n",
            "Class Brush teeth: 50 files\n",
            "Class Book: 50 files\n",
            "X shape: (1400, 50, 35)\n",
            "y shape: (1400,)\n"
          ]
        }
      ],
      "source": [
        "def Dataset(base_file_path, max_files_per_class=50):\n",
        "    # Retrieve all directories in the base file path\n",
        "    data = glob.glob(base_file_path + \"/*\")\n",
        "\n",
        "    # Generate the dictionary keys based on folder names\n",
        "    category_names = [os.path.basename(directory_path) for directory_path in data]\n",
        "    text_files = {category: [] for category in category_names}\n",
        "\n",
        "    # Assign file paths to the respective categories\n",
        "    for directory_path in data:\n",
        "        category = os.path.basename(directory_path)\n",
        "        if category in text_files:\n",
        "            text_files[category].append(directory_path)\n",
        "\n",
        "    # Initialize lists to hold the data and labels\n",
        "    X, y = [], []\n",
        "    class_counts = {key: 0 for key in text_files.keys()}  # Initialize class counts\n",
        "\n",
        "    # Process each category\n",
        "    for label, (key, directory_paths) in enumerate(text_files.items(), start=0):\n",
        "        for directory_path in directory_paths:\n",
        "            files_in_directory = glob.glob(directory_path + \"/*\")\n",
        "            for file_path in files_in_directory:\n",
        "              df = pd.read_csv(file_path, header=None)\n",
        "              df_array = df.iloc[1:51, :35].values # choose the window array that is valid for your application)\n",
        "              #print(df_array.shape, label)\n",
        "              X.append(df_array)\n",
        "              y.append(label)  # Assign numeric label\n",
        "              class_counts[key] += 1  # Increment class count\n",
        "              # Break if reached the maximum number of files for this class\n",
        "              if class_counts[key] == max_files_per_class:\n",
        "                  break\n",
        "            print(f\"Class {key}: {class_counts[key]} files\")\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    y = np.asarray(y, dtype=np.int32)\n",
        "\n",
        "    print(\"X shape:\", X.shape)\n",
        "    print(\"y shape:\", y.shape)\n",
        "\n",
        "    return X, to_categorical(y)\n",
        "# add path\n",
        "file_path = '/content/drive/MyDrive/Csv Fusion'\n",
        "X, y = Dataset(file_path, max_files_per_class=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Split the data**"
      ],
      "metadata": {
        "id": "L6P5nOoFq1hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, x_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(x_tmp, y_tmp, test_size=0.5, random_state=42, stratify=y_tmp)"
      ],
      "metadata": {
        "id": "gOEg_1p-qYUH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "5mvIjeLKrTag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_1D():\n",
        "    \"\"\"\n",
        "    CNN_1D: Class to train a 1D Convolutional Neural Network\n",
        "    Attributes:\n",
        "    filters: List of filters for the convolutional layers\n",
        "    kernels: List of kernel sizes for the convolutional layers\n",
        "    window: Window size for the input data\n",
        "    n_folds: Number of folds for cross-validation\n",
        "    channels: Number of channels in the input data\n",
        "    batch_size: Batch size for training\n",
        "    epochs: Number of epochs for training\n",
        "    learning_rate: Learning rate for the optimizer\n",
        "    optimizer: Optimizer for training\n",
        "    loss: Loss function for training\n",
        "    metrics: Metrics for training\n",
        "    callbacks: Callbacks for training\n",
        "    norm_dict: Dictionary with normalization parameters\n",
        "    train: Method to train the model\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 X_train,\n",
        "                 y_train,\n",
        "                 X_val,\n",
        "                 y_val,\n",
        "                 X_test,\n",
        "                 y_test):\n",
        "        self.filters = [[8], [16]]\n",
        "        self.kernels = [2, 3, 4]\n",
        "        self.batch_size = 64\n",
        "        self.epochs = 300\n",
        "        self.learning_rate = 0.001\n",
        "        self.n_classes = 28\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        self.loss = 'categorical_crossentropy'\n",
        "        self.metrics = ['accuracy']\n",
        "        self.callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
        "                         # tf.keras.callbacks.LearningRateScheduler(schedule=lambda epoch, lr: lr * 0.5 if epoch % 5 == 0 and epoch != 0 and lr>1e-4 else lr)]\n",
        "\n",
        "        # Assign the training, validation, and testing data\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        ########## Train ################\n",
        "        self.train()\n",
        "\n",
        "    def __create_model(self, filters, kernel, n_classes, verbose=1):\n",
        "        model = tf.keras.models.Sequential()\n",
        "        for i, f in enumerate(filters):\n",
        "            if i == 0:\n",
        "                model.add(tf.keras.layers.Conv1D(f, kernel, activation='relu', input_shape=(self.X_train.shape[1:]), padding='same'))\n",
        "            else:\n",
        "                model.add(tf.keras.layers.Conv1D(f, kernel, activation='relu', padding='same'))\n",
        "            model.add(tf.keras.layers.BatchNormalization())\n",
        "            model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "        model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "        # model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
        "        if verbose:\n",
        "            model.summary()\n",
        "        return model\n",
        "    def train(self):\n",
        "        best_val_acc = 0\n",
        "        best_f=None\n",
        "        best_k= None\n",
        "        initial_optimizer_config = self.optimizer.get_config()\n",
        "        with tqdm(total=len(self.filters) * len(self.kernels), desc=\"Training Progress\", unit=\"iteration\") as pbar:\n",
        "            for f in self.filters:\n",
        "                for k in self.kernels:\n",
        "                    model = self.__create_model(f, k, self.n_classes, verbose=0)\n",
        "                    optimizer = type(self.optimizer).from_config(initial_optimizer_config)\n",
        "                    model.compile(optimizer=optimizer, loss=self.loss, metrics=self.metrics)\n",
        "                    history = model.fit(self.X_train,self.y_train, epochs=self.epochs, batch_size=self.batch_size, validation_data=(self.X_val,self.y_val),callbacks=self.callbacks, verbose=0)\n",
        "                    if history.history['val_accuracy'][-1] > best_val_acc:\n",
        "                        # print('best val acc:', history.history['val_accuracy'][-1])\n",
        "                        best_val_acc = history.history['val_accuracy'][-1]\n",
        "                        model.save(f\"model_{f}_{k}.h5\")\n",
        "                        best_f = f\n",
        "                        best_k = k\n",
        "                    pbar.update(1)\n",
        "        best_model = tf.keras.models.load_model(f\"model_{best_f}_{best_k}.h5\")\n",
        "        # best_model.compile(optimizer=self.optimizer, loss=self.loss, metrics=self.metrics)\n",
        "        _, train_accuracy = best_model.evaluate(self.X_train,self.y_train, verbose=0)\n",
        "        _, test_accuracy = best_model.evaluate(self.X_test,self.y_test, verbose=0)\n",
        "        _, val_accuracy = best_model.evaluate(self.X_val,self.y_val, verbose=0)\n",
        "        print(f\"best_f: {best_f},best_k: {best_k}\")\n",
        "        print(\"Best model\")\n",
        "        best_model.summary()\n",
        "        print(\"Train_accuracy:\",train_accuracy,\"Val accuracy:\", val_accuracy, \" Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "7JbPMzjvrcFl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN = CNN_1D(X_train, y_train, X_val, y_val, X_test, y_test)"
      ],
      "metadata": {
        "id": "UxJOhlg6Px9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12665608-eda4-4ca2-e8a0-9e145dfe08d0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 0/6 [00:00<?, ?iteration/s]/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "Training Progress: 100%|██████████| 6/6 [01:40<00:00, 16.73s/iteration]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_f: [16],best_k: 2\n",
            "Best model\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_15 (Conv1D)          (None, 50, 16)            1136      \n",
            "                                                                 \n",
            " batch_normalization_15 (Ba  (None, 50, 16)            64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling1d_15 (MaxPooli  (None, 25, 16)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " global_average_pooling1d_1  (None, 16)                0         \n",
            " 5 (GlobalAveragePooling1D)                                      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 28)                476       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1676 (6.55 KB)\n",
            "Trainable params: 1644 (6.42 KB)\n",
            "Non-trainable params: 32 (128.00 Byte)\n",
            "_________________________________________________________________\n",
            "Train_accuracy: 0.9989795684814453 Val accuracy: 0.9666666388511658  Test Accuracy: 0.9523809552192688\n"
          ]
        }
      ]
    }
  ]
}